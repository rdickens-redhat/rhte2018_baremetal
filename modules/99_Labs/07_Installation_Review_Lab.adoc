:sectnums!:
:hardbreaks:
:scrollbar:
:data-uri:
:toc2:
:toc3:
:showdetailed:
:imagesdir: ./images


== Steps and Templates Used to Deploy the Red Hat OpenStack Platform Environment for the Lab

=== Red Hat OpenStack Platform Overcloud Installation

In this section, you may review the templates used to deploy the overcloud. Red Hat OpenStack Platform version 13 was installed in this lab.

.The following parameters and templates were used to deploy the overcloud:
[%nowrap]
----
#!/bin/bash

openstack overcloud deploy \
--stack overcloud \
--templates /usr/share/openstack-tripleo-heat-templates \
--ntp-server 0.rhel.pool.ntp.org \
-e /usr/share/openstack-tripleo-heat-templates/environments/hyperconverged-ceph.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml  \
-e /home/stack/templates/environments/overcloud_images.yaml \
-e /home/stack/templates/environments/network-environment.yaml \
-e /home/stack/templates/environments/network-isolation.yaml \
-e /home/stack/templates/environments/node-info.yaml \
-e /home/stack/templates/environments/custom-config.yaml \
-e /home/stack/templates/environments/storage-ceph-hyperconverged-environment.yaml \
-e /home/stack/templates/environments/firstboot.yaml \
-e /home/stack/templates/environments/fix-nova-reserved-host-memory.yaml \
-e /home/stack/templates/environments/dns-network-config.yaml \
-e /home/stack/templates/environments/ironic.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/services/ironic.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/services-docker/octavia.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/services/barbican.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/barbican-backend-simple-crypto.yaml  \
-e /usr/share/openstack-tripleo-heat-templates/environments/disable-telemetry.yaml

----

==== Configure the Hyper-Converged Infrastructure (HCI)

In HCI, both the storage area network and the underlying storage abstractions are implemented virtually in software (at or via the hypervisor) rather than physically, in hardware.

Four default environment templates were used to configure the Hyper-Converged Infrastructure.

* `/usr/share/openstack-tripleo-heat-templates/environments/hyperconverged-ceph.yaml`: Enable the Red Hat Ceph Storage in HCI mode and enable the services in controller and compute nodes.
* `/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml`: Enable the Rados Gateway to use the Red Hat Ceph Storage as an Object Storage provider.
* `/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml`: Uses the `ceph-ansible` playbook to deploy the Red Hat Ceph Storage cluster inside the overcloud nodes.
* `/usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml`: Configure Red Hat Ceph Storage RGW as the Cinder backup target.

The custom configuration for Red Hat Ceph Storage used in the lab is defined in the file `storage-ceph-hyperconverged-environment.yaml` inside the `/home/stack/templates/environments/` directory.

*File content*
[%nowrap]
----
resource_registry:
  # StorageMgmt
  OS::TripleO::Network::StorageMgmt: /usr/share/openstack-tripleo-heat-templates/network/storage_mgmt.yaml
  OS::TripleO::Network::External: /usr/share/openstack-tripleo-heat-templates/network/external.yaml
  OS::TripleO::Network::Ports::StorageMgmtVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage_mgmt.yaml
  OS::TripleO::Controller::Ports::StorageMgmtPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage_mgmt.yaml
  OS::TripleO::Compute::Ports::StorageMgmtPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage_mgmt.yaml
  OS::TripleO::Compute::Ports::ExternalPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml

parameter_defaults:
  CephConfigOverrides:
    osd_pool_default_size: 1
    osd_pool_default_min_size: 1
    mon_max_pg_per_osd: 400
  CephAnsibleDisksConfig:
    osd_scenario: collocated
    devices:
      - /dev/vdb
      - /dev/vdc
----

The following parameters are defined in the template:

* The Storage Management network configuration
* The Storage Management port configuration for controller and compute
* Red Hat Ceph Storage parameters to be configured in `ceph.conf` files
+
[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Parameter |Description
| `osd_pool_default_size` | Sets the number of replicas for objects in the pool.
| `osd_pool_default_min_size` | Sets the minimum number of written replicas for objects in the pool in order to acknowledge a write operation to the client.
| `mon_max_pg_per_osd` | Issue a HEALTH_WARN message in the cluster log if the average number of placement groups (PGs) per object storage daemon (OSD) is above this number.
|===
* Configuration for the ceph-ansible playbook, where the disks are defined and the scenario (collocated).
+
In this lab two disks are used (/dev/vdb and /dev/vdc) for the OSD but no disks are used for the journal.

==== Ironic

The overcloud Ironic services were deployed using the template `/usr/share/openstack-tripleo-heat-templates/environments/services/ironic.yaml`.

You may find the following containerized services as part of the Ironic deployment:

[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Container |Description
| `ironic_api` | Takes sanitized API commands from the API controller and performs the actions necessary to fulfill the API request.
| `ironic_conductor` |  Adds/edits/deletes nodes; powers on/off nodes with IPMI or other vendor-specific protocol; provisions/deploys/cleans bare metal nodes.
| `ironic_pxe_http` | Serves the PXE files via HTTP.
| `ironic_pxe_tftp` | Services the PXE files via TFTP.
| `nova_compute` | Runs the Nova compute integrated with Ironic.
|===

An `ironic custom` template, `/home/stack/templates/environments/ironic.yaml` was used in the `overcloud` deployment to provide the custom parameters.
[source,yaml]
----
parameter_defaults:

    NovaSchedulerDefaultFilters:
        - RetryFilter
        - AggregateInstanceExtraSpecsFilter
        - AvailabilityZoneFilter
        - RamFilter
        - DiskFilter
        - ComputeFilter
        - ComputeCapabilitiesFilter
        - ImagePropertiesFilter

    IronicCleaningDiskErase: metadata
----

==== Octavia

The Red Hat OpenStack Platform load balancer service `Octavia` was deployed using the template `/usr/share/openstack-tripleo-heat-templates/environments/services-docker/octavia.yaml`.
 
The following containerized components were deployed as part of the Octavia service:

[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Container |Description
| `octavia_worker` | Takes sanitized API commands from the API controller and performs the actions necessary to fulfill the API request.
| `octavia_api` |  Takes API requests, performs simple sanitizing on them, and sends them to the controller worker over the Oslo messaging bus.
| `octavia_health_manager` | Monitors individual amphorae to ensure they are up and running, and otherwise healthy. It also handles failover events if amphorae fail unexpectedly.
| `octavia_housekeeping` | Cleans up stale (deleted) database records, manages the spares pool, and manages amphora certificate rotation.
|===

==== Barbican

The template used to deploy Barbican is on the path `/usr/share/openstack-tripleo-heat-templates/environments/services/barbican.yaml` to enable the container for the API and
`/usr/share/openstack-tripleo-heat-templates/environments/barbican-backend-simple-crypto.yaml` to enable a simple cryptographic algorithm.

The Barbican environment template deploys three containerized services:

[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Container |Description
| `barbican_worker` | Processes tasks from the queue. Task components are similar to API resources in that they implement business logic, interface with the datastore, and follow on asynchronous tasks as needed.
| `barbican_keystone_listener` | The Barbican service should have its own dedicated notification queue so that it receives all Keystone notifications.
| `barbican_api` | Handles incoming REST requests to Barbican. These nodes can interact with the database directly if the request can be completed synchronously (such as for GET requests), otherwise the queue supports asynchronous processing by worker nodes.
|===

==== Network Configuration

The following custom templates were used to define the network configuration for the overcloud in the lab environment.

`/home/stack/templates/environments/network-environment.yaml`: Defines the network ranges and the VLAN IDs for the overcloud.

*File content*
[%nowrap]
----
resource_registry:
  # NIC Configs for our roles
  OS::TripleO::Compute::Net::SoftwareConfig: ../nic-configs/compute.yaml
  OS::TripleO::Controller::Net::SoftwareConfig: ../nic-configs/controller.yaml

parameter_defaults:
  # Internal API used for private OpenStack Traffic
  InternalApiNetCidr: 172.17.0.0/24
  InternalApiAllocationPools: [{'start': '172.17.0.20', 'end': '172.17.0.200'}]
  InternalApiNetworkVlanID: 20

  # Tenant Network Traffic - will be used for VXLAN over VLAN
  TenantNetCidr: 172.16.0.0/24
  TenantAllocationPools: [{'start': '172.16.0.20', 'end': '172.16.0.200'}]
  TenantNetworkVlanID: 50

  # Public Storage Access - e.g. Nova/Glance <--> Ceph
  StorageNetCidr: 172.20.0.0/24
  StorageAllocationPools: [{'start': '172.20.0.20', 'end': '172.20.0.200'}]

  # Private Storage Access - i.e. Ceph background cluster/replication
  StorageMgmtNetCidr: 172.20.1.0/24
  StorageMgmtAllocationPools: [{'start': '172.20.1.20', 'end': '172.20.1.200'}]

  ExternalNetCidr: 10.0.0.0/24
  # Leave room for floating IPs in the External allocation pool (if required)
  ExternalAllocationPools: [{'start': '10.0.0.20', 'end': '10.0.0.200'}]
  ExternalNetworkVlanID: 10
  # Set to the router gateway on the external network
  ExternalInterfaceDefaultRoute: 10.0.0.1

  # CIDR subnet mask length for provisioning network
  ControlPlaneSubnetCidr: "24"
  # Gateway router for the provisioning network (or Undercloud IP)
  ControlPlaneDefaultRoute: 192.0.2.1
  # Generally the IP of the Undercloud
  EC2MetadataIp: 192.0.2.1
----

`/home/stack/templates/environments/network-isolation.yaml`: Enable the creation of Neutron networks for isolated overcloud traffic and configure the roles to assign ports (related to that role) on the networks.

*File content*
[%nowrap]
----
resource_registry:
  OS::TripleO::Network::External: /usr/share/openstack-tripleo-heat-templates/network/external.yaml
  OS::TripleO::Network::InternalApi: /usr/share/openstack-tripleo-heat-templates/network/internal_api.yaml
  OS::TripleO::Network::Storage: /usr/share/openstack-tripleo-heat-templates/network/storage.yaml
  OS::TripleO::Network::Tenant: /usr/share/openstack-tripleo-heat-templates/network/tenant.yaml
  # Management network is optional and disabled by default.
  # To enable it, include environments/network-management.yaml
  OS::TripleO::Network::Management: /usr/share/openstack-tripleo-heat-templates/network/management.yaml

  # Port assignments for the VIPs
  OS::TripleO::Network::Ports::ExternalVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml
  OS::TripleO::Network::Ports::InternalApiVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api.yaml
  OS::TripleO::Network::Ports::StorageVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage.yaml
  OS::TripleO::Network::Ports::RedisVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/vip.yaml

  # Port assignments for the controller role
  OS::TripleO::Controller::Ports::ExternalPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml
  OS::TripleO::Controller::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api.yaml
  OS::TripleO::Controller::Ports::StoragePort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage.yaml
  OS::TripleO::Controller::Ports::TenantPort: /usr/share/openstack-tripleo-heat-templates/network/ports/tenant.yaml
  OS::TripleO::Controller::Ports::ManagementPort: /usr/share/openstack-tripleo-heat-templates/network/ports/management.yaml

  # Port assignments for the compute role
  OS::TripleO::Compute::Ports::ExternalPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml
  OS::TripleO::Compute::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api.yaml
  OS::TripleO::Compute::Ports::StoragePort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage.yaml
  OS::TripleO::Compute::Ports::TenantPort: /usr/share/openstack-tripleo-heat-templates/network/ports/tenant.yaml
  OS::TripleO::Compute::Ports::ManagementPort: /usr/share/openstack-tripleo-heat-templates/network/ports/management.yaml
----

`/home/stack/templates/nic-configs/compute.yaml` and `/home/stack/templates/nic-configs/controller.yaml`: Define the interfaces/bridge for the controller and the compute nodes and its attached networks.

*File content*
[%nowrap]
----
heat_template_version: queens

parameters:
  ControlPlaneIp:
    default: ''
    description: IP address/subnet on the ctlplane network
    type: string
  ExternalIpSubnet:
    default: ''
    description: IP address/subnet on the external network
    type: string
  InternalApiIpSubnet:
    default: ''
    description: IP address/subnet on the internal_api network
    type: string
  StorageIpSubnet:
    default: ''
    description: IP address/subnet on the storage network
    type: string
  StorageMgmtIpSubnet:
    default: ''
    description: IP address/subnet on the storage_mgmt network
    type: string
  TenantIpSubnet:
    default: ''
    description: IP address/subnet on the tenant network
    type: string
  ManagementIpSubnet: # Only populated when including environments/network-management.yaml
    default: ''
    description: IP address/subnet on the management network
    type: string
  BondInterfaceOvsOptions:
    default: 'bond_mode=active-backup'
    description: The ovs_options string for the bond interface. Set things like
                 lacp=active and/or bond_mode=balance-slb using this option.
    type: string
  ExternalNetworkVlanID:
    default: 10
    description: Vlan ID for the external network traffic.
    type: number
  InternalApiNetworkVlanID:
    default: 20
    description: Vlan ID for the internal_api network traffic.
    type: number
  StorageNetworkVlanID:
    default: 30
    description: Vlan ID for the storage network traffic.
    type: number
  StorageMgmtNetworkVlanID:
    default: 40
    description: Vlan ID for the storage_mgmt network traffic.
    type: number
  TenantNetworkVlanID:
    default: 50
    description: Vlan ID for the tenant network traffic.
    type: number
  ManagementNetworkVlanID:
    default: 60
    description: Vlan ID for the management network traffic.
    type: number
  ExternalInterfaceDefaultRoute:
    default: '10.0.0.1'
    description: default route for the external network
    type: string
  ControlPlaneSubnetCidr: # Override this via parameter_defaults
    default: '24'
    description: The subnet CIDR of the control plane network.
    type: string
  ControlPlaneDefaultRoute: # Override this via parameter_defaults
    description: The default route of the control plane network.
    type: string
  DnsServers: # Override this via parameter_defaults
    default: []
    description: A list of DNS servers (2 max for some implementations) that will be added to resolv.conf.
    type: comma_delimited_list
  EC2MetadataIp: # Override this via parameter_defaults
    description: The IP address of the EC2 metadata server.
    type: string

resources:
  OsNetConfigImpl:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config:
        str_replace:
          template:
            get_file: /usr/share/openstack-tripleo-heat-templates/network/scripts/run-os-net-config.sh
            params:
              $network_config:
                network_config:
                - type: ovs_bridge
                  name: br-baremetal
                  use_dhcp: false
                  members:
                    -
                      type: interface
                      name: nic5

                - type: interface
                  name: nic1
                  mtu: 1500
                  use_dhcp: false
                  addresses:
                  - ip_netmask:
                      list_join:
                      - /
                      - - {get_param: ControlPlaneIp}
                        - {get_param: ControlPlaneSubnetCidr}
              -
                type: ovs_bridge
                name: bridge_name
                use_dhcp: false
                members:
                  -
                    type: interface
                    name: nic2

              -
                type: vlan
                vlan_id: {get_param: ExternalNetworkVlanID}
                device: br-ex
                addresses:
                  -
                    ip_netmask: {get_param: ExternalIpSubnet}
                routes:
                - default: true
                  next_hop: {get_param: ExternalInterfaceDefaultRoute}

              -
                type: vlan
                vlan_id: {get_param: InternalApiNetworkVlanID}
                device: br-ex
                addresses:
                  -
                    ip_netmask: {get_param: InternalApiIpSubnet}
              -
                type: vlan
                vlan_id: {get_param: TenantNetworkVlanID}
                device: br-ex
                addresses:
                  -
                    ip_netmask: {get_param: TenantIpSubnet}

              -
                type: ovs_bridge
                name: br-storage
                use_dhcp: false
                mtu: 1500
                addresses:
                  - ip_netmask: {get_param: StorageIpSubnet}
                members:
                  -
                    type: interface
                    name: nic3

              -
                type: ovs_bridge
                name: br-storage-mgmt
                use_dhcp: false
                mtu: 9000
                addresses:
                  - ip_netmask: {get_param: StorageMgmtIpSubnet}
                members:
                  -
                    type: interface
                    name: nic4

outputs:
  OS::stack_id:
    description: The OsNetConfigImpl resource.
    value: {get_resource: OsNetConfigImpl}
----

*Network interfaces in the overcloud nodes*
[cols="1,1,1,2",options="header",caption="",options="nowrap"]
|===
| Interface | OpenStack Name |OVS Bridge |Description
| `eth0` | `nic1` | N/D | Connected to the provisioning network (DHCP from 192.0.2.0/24)
| `eth1` | `nic2` | br-ex | Connected to a trunk (VLANs for internal API, external and tenant)
| `eth2` | `nic3` | br-storage | Connected to the storage network.
| `eth3` | `nic4` | br-storage-mgmt | Connected to the storage management network.
|===

==== Custom Configuration Templates

The following additional custom templates were used to provision the lab.

`/home/stack/templates/environments/node-info.yaml`: Specifies the number of nodes per role and their flavors.

*File content*
[%nowrap]
----
parameter_defaults:
  ControllerCount: 3
  ComputeCount: 2
  OvercloudControllerFlavor: control
  OvercloudComputeFlavor: compute
----

`/home/stack/templates/heat/firstboot.yaml`: Defines the user data to be used during the first boot on all the overcloud nodes.

*File content*
[%nowrap]
----
heat_template_version: 2014-10-16

description: >
  Set root password

resources:
  userdata:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: set_pass}

  set_pass:
    type: OS::Heat::SoftwareConfig
    properties:
      config: |
        #!/bin/bash
        echo 'r3dh4t1!' | passwd --stdin root

outputs:
  OS::stack_id:
    value: {get_resource: userdata}
----

`/home/stack/templates/environments/firstboot.yaml`: Defines the `userdata` resource, which sets the `root` password.

*File content*
[%nowrap]
----
resource_registry:
  OS::TripleO::NodeUserData: ../heat/firstboot.yaml
----


`/home/stack/templates/environments/custom-config.yaml`: Miscellaneous configuration for the overcloud.

*File content*
[%nowrap]
----
parameter_defaults:
  OctaviaAmphoraSshKeyFile: /tmp/id_rsa.pub <1>
  BarbicanSimpleCryptoGlobalDefault: true <2>
  TimeZone: 'America/New_York' <3>
  ServiceNetMap: <4>
    KeystoneAdminApiNetwork: external
----
<1> The file specified by the parameter `OctaviaAmphoraSshKeyFile` must be readable by the `mistral` user on the undercloud server.
<2> Enable the `Simple Crypto` globally.
<3> Defines the timezone.
<4> Change the network for the `Keystone` administration network.

[IMPORTANT]
The endpoint for the Keystone Admin was moved to external due to integration with Red Hat OpenShift.

==== DNS Configuration Templates

`/home/stack/templates/environments/dns-network-config.yaml`: Template to configure the DNS for the overcloud nodes

*File content*
[%nowrap]
----
parameter_defaults:
  CloudName: openstack.example.com
  CloudDomain: example.com
  # Define the DNS servers (maximum 2) for the overcloud nodes
  DnsServers: ['192.0.2.254']
----

`/home/stack/templates/environments/fix-nova-reserved-host-memory.yaml`: Custom parameters to override the lab environment's limitation

*File content*
[%nowrap]
----
parameter_defaults:
  NovaReservedHostMemory: 1024
----
+
[NOTE]
By default, the Red Hat OpenStack Platform director reserves `4 GB` of the compute nodes' memory for host processes. This may not feasible for small scale deployments. The `NovaReservedHostMemory` parameter limits the reserved host memory to `1 GB`.


==== Disable Metrics

The environment file `/usr/share/openstack-tripleo-heat-templates/environments/disable-telemetry.yaml` disables the Ceilometer, Gnocchi, Aodh and Panko services to restrict the overcloud resource utilization.

=== Deploy a DNS instance to be used for Red Hat OpenShift Container Platform

For the DNS VM's installation the playbooks hosted on `https://github.com/tomassedovic/devns` were used.

With this DNS VM server it is possible to dynamically update the DNS using `nsupdate` and is used by the OpenShift Container Platform installation to generate the required DNS records.

The following steps were used for the DNS server's installation:

. Configure file `/home/stack/devns/vars.yaml`.
+
*File content*
+
[%nowrap]
----
---
dns_domain: openshift.example.com

external_network: public

# openstack keypair list
key_name: openshift

# openstack image list
image: rhel7

# openstack flavor list
flavor: m1.small2

server_name: openshift-dns

dns_forwarders: ["8.8.8.8"]
----

. Install `shade` package for Python.

We need to install the `shade` library as a prerequisite to install OpenShift Container Platform on Red Hat OpenStack Platform. Shade is a simple client library for interacting with Red Hat OpenStack Platform clouds.
+
[%nowrap]
----
(overcloud) [stack@undercloud ~]$ sudo yum install python2-shade
----
+
*Sample output*
[%nowrap]
----
Loaded plugins: product-id, search-disabled-repos, subscription-manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
Resolving Dependencies
--> Running transaction check
---> Package python2-shade.noarch 0:1.27.1-1.el7ost will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==============================================================================================================================================================================================
 Package                                   Arch                               Version                                       Repository                                                   Size
==============================================================================================================================================================================================
Installing:
 python2-shade                             noarch                             1.27.1-1.el7ost                               rhel-7-server-openstack-13-rpms                             552 k

Transaction Summary
==============================================================================================================================================================================================
Install  1 Package

Total download size: 552 k
Installed size: 3.1 M
Is this ok [y/d/N]: y
Downloading packages:
python2-shade-1.27.1-1.el7ost.noarch.rpm                                                                                                                               | 552 kB  00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : python2-shade-1.27.1-1.el7ost.noarch                                                                                                                                       1/1
  Verifying  : python2-shade-1.27.1-1.el7ost.noarch                                                                                                                                       1/1

Installed:
  python2-shade.noarch 0:1.27.1-1.el7ost

Complete!
----

. Deploy the VMs using Ansible Playbook
+
Once the `vars.yaml` file is configured, we only need to run the `deploy.yaml` playbook to create:

* Network with name `openshift-dns`
* Subnet with name `openshift-dns` with range `192.168.23.0/24`
* Router connected to the `public` network with name `openshift-dns`
* VM with name `openshift-dns`

.Run the `deploy.yaml` playbook
[%nowrap]
----
(overcloud) [stack@undercloud ~]$ cd devns/
(overcloud) [stack@undercloud ~]$ ansible-playbook --private-key ~/.ssh/id_rsa --user cloud-user deploy.yaml -e @vars.yaml
----
+
When the deployment finishes, it shows the `Floating IP`, `key algorithm` and `key secret`  used to update DNS dynamically.


=== Red Hat OpenShift Container Platform Installation

In this section, you may review the installation steps used to deploy OpenShift Container Platform in the lab environment.

Red Hat OpenShift Container Platform 3.10 was installed on top of Red Hat OpenStack using the `openshift-ansible` playbooks. 


==== Review the OpenShift Container Platform Installation Variables

The Ansible inventory file `/root/hosts` contains the `node` configuration and parameters used for the OpenShift Container Platform  installation.

*File content*
[%nowrap]
----
[masters]
ocp-master01.openshift.example.com
[etcd]
ocp-master01.openshift.example.com
[nodes]
ocp-master01.openshift.example.com openshift_ip=192.0.3.16 ansible_host=192.0.3.16 openshift_node_group_name='node-config-master'
ocp-infra01.openshift.example.com openshift_ip=192.0.3.22 ansible_host=192.0.3.22 openshift_node_group_name='node-config-infra'
ocp-node01.openshift.example.com openshift_ip=192.0.3.12 ansible_host=192.0.3.12 openshift_node_group_name='node-config-compute'
[new_nodes]
ocp-node02.openshift.example.com openshift_ip=192.0.3.14 ansible_host=192.0.3.14 openshift_node_group_name='node-config-compute'
[OSEv3:children]
masters
nodes
new_nodes
[OSEv3:vars]
ansible_user=cloud-user
ansible_become=yes
openshift_deployment_type=openshift-enterprise
openshift_release="3.10"
openshift_master_default_subdomain=apps.openshift.example.com
openshift_master_cluster_hostname=console.openshift.example.com
debug_level=2
openshift_disable_check=disk_availability,memory_availability,docker_storage,package_availability,package_version
openshift_additional_repos=[{'id': 'ose-repo', 'name': 'rhel-7-server-ose-3.10-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-server-ose-3.10-rpms', 'enabled': 1, 'gpgcheck': 0},{'id': 'rhel-7-server-rpms-repo', 'name': 'rhel-7-server-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-server-rpms', 'enabled': 1, 'gpgcheck': 0},{'id': 'rhel-7-server-extras-rpms-repo', 'name': 'rhel-7-server-extras-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-server-extras-rpms', 'enabled': 1, 'gpgcheck': 0},{'id': 'rhel-7-fast-datapath-rpms', 'name': 'rhel-7-fast-datapath-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-fast-datapath-rpms', 'enabled': 1, 'gpgcheck': 0}]
openshift_cloudprovider_kind=openstack
openshift_cloudprovider_openstack_auth_url="http://10.0.0.26:5000//v3"
openshift_cloudprovider_openstack_username="admin"
openshift_cloudprovider_openstack_password="Eu3xKG6UKpKvZReEFc7FKqCn6"
openshift_cloudprovider_openstack_tenant_name="admin"
openshift_cloudprovider_openstack_region="regionOne"
openshift_cloudprovider_openstack_domain_name="Default"
openshift_cloudprovider_openstack_blockstorage_version=v2
osm_default_node_selector='region=primary'
openshift_hosted_router_selector='node-role.kubernetes.io/infra=true'
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
openshift_enable_service_catalog=false
template_service_broker_install=false
openshift_ca_cert_expire_days=1825
openshift_node_cert_expire_days=730
openshift_master_cert_expire_days=730
etcd_ca_default_days=1825
----

==== OpenShift Container Platform Installation via Playbook

OpenShift Container Platform uses the playbooks provided in the `openshift-ansible` package for deployment and administration. After installing the `openshift-ansible` package you can find the playbooks for various tasks under the `/usr/share/ansible/openshift-ansible/playbooks/` directory.

.To deploy the OpenShift Container Platform cluster you may run the playbook with the required `inventory` file (hosts) as the input.
[%nowrap]
----
[root@ocp-bastion ~]# ansible-playbook -i <inventory file path> /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml
----
